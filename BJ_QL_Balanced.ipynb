{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "qzHi70zwCx6a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVWQ9ZEd7jx8",
        "outputId": "a87bebef-5477-43ac-bad2-7f06784893b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/300000, Win Rate: 37.78%, Epsilon: 1.0000\n",
            "Episode 50000/300000, Win Rate: 43.62%, Epsilon: 0.0846\n",
            "Episode 100000/300000, Win Rate: 43.20%, Epsilon: 0.0799\n",
            "Episode 150000/300000, Win Rate: 42.42%, Epsilon: 0.0774\n",
            "Episode 200000/300000, Win Rate: 42.68%, Epsilon: 0.0757\n",
            "Episode 250000/300000, Win Rate: 43.60%, Epsilon: 0.0745\n",
            "Performance sur 5000 épisodes: 42.30% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.423"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Agent Q-Learning optimisé pour un bon équilibre exploration/exploitation.\n",
        "        #- Décroissance logarithmique de epsilon\n",
        "        #- Décroissance exponentielle de alpha\n",
        "        #- Initialisation biaisée pour accélérer l'apprentissage\n",
        "\n",
        "# Paramètres globaux\n",
        "gamma= 0.97\n",
        "alpha_start= 0.1\n",
        "epsilon_start= 1.0\n",
        "epsilon_min= 0.05\n",
        "epsilon_decay= 0.00005\n",
        "q_table= defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits= defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "def choose_action(state, episode):\n",
        "    # Politique epsilon-greedy avec une décroissance logarithmique de epsilon\n",
        "    if np.random.rand()< epsilon_start / (1 + np.log(1 + episode)):\n",
        "        return env.action_space.sample()\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, done, episode):\n",
        "    # Initialisation biaisée pour un apprentissage plus rapide\n",
        "    if state not in q_table:\n",
        "        q_table[state]= np.array([0.1, 0.2])\n",
        "\n",
        "    # Alpha décroissant exponentiellement\n",
        "    alpha= max(0.02, alpha_start * np.exp(-0.00005 * episode))\n",
        "\n",
        "    # Mise à jour Q-Learning\n",
        "    best_next_action= np.argmax(q_table[next_state])\n",
        "    target= reward + gamma* q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha* (target- q_table[state][action])\n",
        "\n",
        "    visits[state][action] +=1\n",
        "\n",
        "def train(num_episodes=300000):\n",
        "    for episode in range(num_episodes):\n",
        "        state, _= env.reset()\n",
        "        done= False\n",
        "\n",
        "        while not done:\n",
        "            action= choose_action(state, episode)\n",
        "            next_state, reward, terminated, truncated, _= env.step(action)\n",
        "            done= terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done, episode)\n",
        "            state= next_state\n",
        "\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate= evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}, Epsilon: {epsilon_start / (1 + np.log(1 + episode)):.4f}\")\n",
        "\n",
        "def evaluate(num_episodes=5000, verbose=True):\n",
        "    wins, losses, draws= 0, 0, 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state, _= env.reset()\n",
        "        done= False\n",
        "\n",
        "        while not done:\n",
        "            action= np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _= env.step(action)\n",
        "            done= terminated or truncated\n",
        "            state= next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins+= 1\n",
        "        elif reward < 0:\n",
        "            losses+= 1\n",
        "        else:\n",
        "            draws+= 1\n",
        "\n",
        "    win_rate= wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "env= gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "# Entraînement avec un équilibre stable\n",
        "train(num_episodes=300000)\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v2\n",
        "#Agent Q-Learning optimisé pour un bon équilibre exploration/exploitation.\n",
        "        #- Décroissance progressive de epsilon avec exploration forcée\n",
        "        #- Décroissance lente de alpha pour stabiliser l'apprentissage\n",
        "        #- Ajustement de gamma pour favoriser des décisions plus pragmatiques\n",
        "       # - Warm Start amélioré pour accélérer l'apprentissage des stratégies sûres\n",
        "\n",
        "#Paramètres globaux\n",
        "gamma= 0.95\n",
        "alpha_start= 0.1\n",
        "epsilon_start= 1.0\n",
        "epsilon_min= 0.05\n",
        "epsilon_decay= 0.00002\n",
        "q_table= defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits= defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "def choose_action(state, episode):\n",
        "    # Phase d'exploration forcée pour les 50 000 premiers épisodes\n",
        "    if episode<50000:\n",
        "        epsilon= 1.0\n",
        "    else:\n",
        "        epsilon= max(epsilon_min, epsilon_start / (1+ np.log(1+episode - 50000)))\n",
        "\n",
        "    if np.random.rand()< epsilon:\n",
        "        return env.action_space.sample()\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, done, episode):\n",
        "    # Initialisation biaisée améliorée pour un apprentissage plus rapide\n",
        "    if state not in q_table:\n",
        "        q_table[state]= np.array([0.3, 0.4])\n",
        "\n",
        "    # Alpha décroissant très lentement pour assurer un apprentissage sur le long terme\n",
        "    alpha= max(0.02, alpha_start / (1+ 0.00002*episode))\n",
        "\n",
        "    best_next_action= np.argmax(q_table[next_state])\n",
        "    target= reward + gamma* q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha* (target- q_table[state][action])\n",
        "\n",
        "    visits[state][action] +=1\n",
        "\n",
        "def train(num_episodes=500000):\n",
        "    for episode in range(num_episodes):\n",
        "        state, _= env.reset()\n",
        "        done= False\n",
        "\n",
        "        while not done:\n",
        "            action= choose_action(state, episode)\n",
        "            next_state, reward, terminated, truncated, _= env.step(action)\n",
        "            done= terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done, episode)\n",
        "            state= next_state\n",
        "\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate= evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(num_episodes=5000, verbose=True):\n",
        "    wins, losses, draws= 0, 0, 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state, _= env.reset()\n",
        "        done= False\n",
        "\n",
        "        while not done:\n",
        "            action= np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _= env.step(action)\n",
        "            done= terminated or truncated\n",
        "            state= next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins+= 1\n",
        "        elif reward < 0:\n",
        "            losses+= 1\n",
        "        else:\n",
        "            draws+= 1\n",
        "\n",
        "    win_rate= wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "env= gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "# Entraînement avec un équilibre stable\n",
        "train(num_episodes=300000)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm_UG2n18j9G",
        "outputId": "b8ddce90-5708-4eb7-b57d-f893fd8aa556"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/300000, Win Rate: 38.26%\n",
            "Episode 50000/300000, Win Rate: 42.14%\n",
            "Episode 100000/300000, Win Rate: 42.12%\n",
            "Episode 150000/300000, Win Rate: 42.48%\n",
            "Episode 200000/300000, Win Rate: 41.76%\n",
            "Episode 250000/300000, Win Rate: 43.98%\n",
            "Performance sur 5000 épisodes: 43.62% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4362"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v3\n",
        "#Agent Q-Learning ultra-optimisé pour maximiser les victoires.\n",
        "        #- Facteur UCB pour favoriser l'exploration des actions sous-évaluées\n",
        "        #- Réduction plus lente de alpha pour améliorer l'apprentissage long terme\n",
        "        #- Ajustement gamma pour décisions plus offensives\n",
        "        #- Warm Start amélioré\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "#Paramètres d'apprentissage\n",
        "alpha_start= 0.1\n",
        "gamma = 0.92\n",
        "epsilon_start= 1.0\n",
        "epsilon_min= 0.05\n",
        "epsilon_decay= 0.00002\n",
        "\n",
        "#Q-Table et visites\n",
        "q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "def choose_action(state, episode):\n",
        "    #Politique UCB + epsilon-greedy pour meilleure exploration\n",
        "    if np.random.rand()< epsilon_start:\n",
        "        return env.action_space.sample()  #Exploration\n",
        "\n",
        "    #Facteur UCB pour favoriser les actions sous-explorées\n",
        "    total_visits = np.sum(visits[state]) + 1e-6\n",
        "    ucb_values = q_table[state] + np.sqrt(2 * np.log(total_visits) / (visits[state] + 1e-6))\n",
        "    return np.argmax(ucb_values)  #Exploitation améliorée\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, done, episode):\n",
        "    #Mise à jour de la Q-table avec réduction optimisée de alpha et epsilon\n",
        "    #Warm Start ajusté\n",
        "    if state not in q_table:\n",
        "        q_table[state] = np.array([0.5, 0.7])  #Influence plus les premières décisions\n",
        "\n",
        "    alpha = max(0.02, alpha_start / (1 + 0.00001 * episode))\n",
        "\n",
        "    if episode < 50000:\n",
        "        epsilon = 1.0\n",
        "    else:\n",
        "        epsilon = max(epsilon_min, epsilon_start / (1 + np.log(1 + episode - 50000)))\n",
        "\n",
        "    best_next_action = np.argmax(q_table[next_state])\n",
        "    target = reward + gamma * q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "    visits[state][action] += 1\n",
        "\n",
        "def train(num_episodes=600000):\n",
        "    #Entraîne l'agent Q-Learning avec une stabilité renforcée\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = choose_action(state, episode)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done, episode)\n",
        "            state = next_state\n",
        "\n",
        "        #Performances tous les 50 000 épisodes\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate = evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(num_episodes=5000, verbose=True):\n",
        "    #Évalue l'agent après l'entraînement\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "#Entraînement final\n",
        "train(num_episodes=600000)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d_Jt7yI_fcC",
        "outputId": "3524aa15-55c7-48d3-f213-602c466e9770"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/600000, Win Rate: 38.28%\n",
            "Episode 50000/600000, Win Rate: 42.70%\n",
            "Episode 100000/600000, Win Rate: 43.94%\n",
            "Episode 150000/600000, Win Rate: 42.88%\n",
            "Episode 200000/600000, Win Rate: 43.52%\n",
            "Episode 250000/600000, Win Rate: 42.56%\n",
            "Episode 300000/600000, Win Rate: 44.68%\n",
            "Episode 350000/600000, Win Rate: 43.48%\n",
            "Episode 400000/600000, Win Rate: 44.22%\n",
            "Episode 450000/600000, Win Rate: 43.86%\n",
            "Episode 500000/600000, Win Rate: 41.68%\n",
            "Episode 550000/600000, Win Rate: 43.28%\n",
            "Performance sur 5000 épisodes: 42.16% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4216"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v4\n",
        "\n",
        "#Agent Q-Learning amélioré pour dépasser 45% :\n",
        "#- Phase de réexploration après 300 K épisodes\n",
        "#- Décroissance plus lente de alpha pour un apprentissage long terme\n",
        "#- Exploration légère en fin de partie pour ajustement\n",
        "\n",
        "# Définition de l'agent\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.92\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.00002\n",
        "method = \"q_learning\"\n",
        "\n",
        "agent = {}\n",
        "agent['env'] = env\n",
        "agent['alpha_start'] = alpha\n",
        "agent['gamma'] = gamma\n",
        "agent['epsilon'] = epsilon_start\n",
        "agent['epsilon_start'] = epsilon_start\n",
        "agent['epsilon_min'] = epsilon_min\n",
        "agent['epsilon_decay'] = epsilon_decay\n",
        "agent['method'] = method\n",
        "agent['q_table'] = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "agent['visits'] = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "def choose_action(agent, state, episode):\n",
        "    if np.random.rand() < agent['epsilon']:\n",
        "        return agent['env'].action_space.sample()\n",
        "\n",
        "    if episode > 500000 and np.random.rand() < 0.02:  # Perturbation contrôlée\n",
        "        return agent['env'].action_space.sample()\n",
        "\n",
        "    return np.argmax(agent['q_table'][state])\n",
        "\n",
        "def update_q_table(agent, state, action, reward, next_state, next_action=None, done=False, episode=1):\n",
        "    if state not in agent['q_table']:\n",
        "        agent['q_table'][state] = np.array([0.5, 0.7])\n",
        "\n",
        "    alpha = max(0.02, agent['alpha_start'] / (1 + 0.000005 * episode))\n",
        "\n",
        "    if 300000 <= episode < 450000:\n",
        "        agent['epsilon'] = max(agent['epsilon_min'], 0.15)\n",
        "    else:\n",
        "        agent['epsilon'] = max(agent['epsilon_min'], agent['epsilon_start'] / (1 + np.log(1 + episode)))\n",
        "\n",
        "    best_next_action = np.argmax(agent['q_table'][next_state])\n",
        "    target = reward + agent['gamma'] * agent['q_table'][next_state][best_next_action] * (not done)\n",
        "    agent['q_table'][state][action] += alpha * (target - agent['q_table'][state][action])\n",
        "\n",
        "    agent['visits'][state][action] += 1\n",
        "\n",
        "def train(agent, num_episodes=700000):\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = agent['env'].reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = choose_action(agent, state, episode)\n",
        "            next_state, reward, terminated, truncated, _ = agent['env'].step(action)\n",
        "            done = terminated or truncated\n",
        "            update_q_table(agent, state, action, reward, next_state, done=done, episode=episode)\n",
        "            state = next_state\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate = evaluate(agent, 5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(agent, num_episodes=5000, verbose=True):\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = agent['env'].reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(agent['q_table'][state])\n",
        "            next_state, reward, terminated, truncated, _ = agent['env'].step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "#Entraînement final\n",
        "train(agent, num_episodes=700000)\n",
        "evaluate(agent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwFiLqp-B3YU",
        "outputId": "1a6d45b7-092d-45da-f3a8-fcf6b84fdbe5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/700000, Win Rate: 38.06%\n",
            "Episode 50000/700000, Win Rate: 42.26%\n",
            "Episode 100000/700000, Win Rate: 41.84%\n",
            "Episode 150000/700000, Win Rate: 43.34%\n",
            "Episode 200000/700000, Win Rate: 41.60%\n",
            "Episode 250000/700000, Win Rate: 42.30%\n",
            "Episode 300000/700000, Win Rate: 42.24%\n",
            "Episode 350000/700000, Win Rate: 42.76%\n",
            "Episode 400000/700000, Win Rate: 42.28%\n",
            "Episode 450000/700000, Win Rate: 42.72%\n",
            "Episode 500000/700000, Win Rate: 41.78%\n",
            "Episode 550000/700000, Win Rate: 43.04%\n",
            "Episode 600000/700000, Win Rate: 42.78%\n",
            "Episode 650000/700000, Win Rate: 44.02%\n",
            "Performance sur 5000 épisodes: 42.54% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4254"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}