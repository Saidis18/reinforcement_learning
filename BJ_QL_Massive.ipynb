{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "R6-ngBwCEjZV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G58ubJJGEOh9",
        "outputId": "1f3b581e-6742-4f21-ce19-cfc1b19f1470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/700000, Win Rate: 37.52%\n",
            "Episode 50000/700000, Win Rate: 42.48%\n",
            "Episode 100000/700000, Win Rate: 43.26%\n",
            "Episode 150000/700000, Win Rate: 42.22%\n",
            "Episode 200000/700000, Win Rate: 42.66%\n",
            "Episode 250000/700000, Win Rate: 43.90%\n",
            "Episode 300000/700000, Win Rate: 44.14%\n",
            "Episode 350000/700000, Win Rate: 42.02%\n",
            "Episode 400000/700000, Win Rate: 41.48%\n",
            "Episode 450000/700000, Win Rate: 42.06%\n",
            "Episode 500000/700000, Win Rate: 41.82%\n",
            "Episode 550000/700000, Win Rate: 41.34%\n",
            "Episode 600000/700000, Win Rate: 42.32%\n",
            "Episode 650000/700000, Win Rate: 43.86%\n",
            "Performance sur 5000 épisodes: 43.26% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4326"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#v1\n",
        "\n",
        "#Initialisation de l'agent Q-Learning optimisé pour une phase massive d'exploration au départ\n",
        "# - Phase d'exploration forcée jusqu'à 200K épisodes\n",
        "# - Décroissance hybride de epsilon (exponentielle + logarithmique)\n",
        "# - Apprentissage hybride (phase rapide suivie de stabilisation)\n",
        "\n",
        "#Création de l'environnement\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "#Paramètres de l'agent\n",
        "alpha_start = 0.1\n",
        "gamma = 0.95\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.00002\n",
        "method = \"q_learning\"\n",
        "\n",
        "q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "def choose_action(state, episode):\n",
        "    #Politique epsilon-greedy avec une phase d'exploration massive au début.\n",
        "\n",
        "    #Exploration massive pendant les 200K premiers épisodes\n",
        "    if episode < 200000:\n",
        "        epsilon = 1.0\n",
        "    else:\n",
        "        epsilon = max(epsilon_min, epsilon_start / (1 + np.log(1 + episode - 200000)))\n",
        "\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, next_action=None, done=False, episode=1):\n",
        "\n",
        "#Mise à jour de la Q-table avec une hybridation de la réduction de alpha.\n",
        "\n",
        "    #Warm Start amélioré pour influencer les premières décisions\n",
        "    if state not in q_table:\n",
        "        q_table[state] = np.array([0.6, 0.8])  # Favorise Stand légèrement au début\n",
        "\n",
        "    #Alpha hybride : Phase rapide + stabilisation plus lente après 300K épisodes\n",
        "    if episode < 300000:\n",
        "        alpha = max(0.02, alpha_start / (1 + 0.00001 * episode))\n",
        "    else:\n",
        "        alpha = max(0.02, alpha_start * np.exp(-0.000005 * (episode - 300000)))\n",
        "\n",
        "    #Epsilon : Mélange exponentiel + logarithmique après 200K épisodes\n",
        "    if episode < 200000:\n",
        "        epsilon = 1.0\n",
        "    else:\n",
        "        epsilon = max(epsilon_min, epsilon_start * np.exp(-epsilon_decay * (episode - 200000)))\n",
        "\n",
        "    # Mise à jour Q-Learning\n",
        "    best_next_action = np.argmax(q_table[next_state])\n",
        "    target = reward + gamma * q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "\n",
        "    visits[state][action] += 1\n",
        "\n",
        "def train(num_episodes=700000):\n",
        "    #Entraîne l'agent avec une exploration massive suivie d'une exploitation intelligente.\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = choose_action(state, episode)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done=done, episode=episode)\n",
        "            state = next_state\n",
        "\n",
        "        #Suivi des performances tous les 50 000 épisodes\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate = evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(num_episodes=5000, verbose=True):\n",
        "    #Évalue l'agent après l'entraînement.\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "#Entraînement de l'agent\n",
        "train(num_episodes=700000)\n",
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v2\n",
        "\n",
        "#Dernière version optimisée du Q-Learning :\n",
        " #- Augmentation de gamma pour décisions stratégiques\n",
        " #- Réduction plus forte de epsilon après 500K épisodes\n",
        " #- Meilleure pondération des récompenses pour un apprentissage plus efficace\n",
        "\n",
        "# Initialisation des paramètres de l'agent\n",
        "alpha_start = 0.1\n",
        "gamma = 0.97  # Encourage un apprentissage plus stratégique\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.00002\n",
        "method = \"q_learning\"\n",
        "\n",
        "# Création de l'environnement\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "epsilon = epsilon_start\n",
        "\n",
        "def choose_action(state, episode):\n",
        "    \"\"\"\n",
        "    Politique epsilon-greedy avec ajustement dynamique après 500K épisodes.\n",
        "    \"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, next_action=None, done=False, episode=1, method=\"q_learning\"):\n",
        "    \"\"\"\n",
        "    Mise à jour de la Q-table avec renforcement des récompenses et stabilisation après 500K épisodes.\n",
        "    \"\"\"\n",
        "    # Warm Start conservé\n",
        "    if state not in q_table:\n",
        "        q_table[state] = np.array([0.5, 0.7])\n",
        "\n",
        "    # Alpha diminue plus lentement\n",
        "    alpha = max(0.02, alpha_start / (1 + 0.000005 * episode))\n",
        "\n",
        "    # Réduction drastique de epsilon après 500K épisodes (Focus Mode)\n",
        "    global epsilon\n",
        "    if episode > 500000:\n",
        "        epsilon = max(epsilon_min, 0.02)\n",
        "\n",
        "    # Augmenter l’impact des victoires et défaites\n",
        "    if reward > 0:\n",
        "        reward = 2  # Double la valeur des victoires\n",
        "    elif reward < 0:\n",
        "        reward = -2  # Pénalise plus les défaites\n",
        "\n",
        "    if method == \"sarsa\":\n",
        "        # Mise à jour SARSA\n",
        "        target = reward + gamma * q_table[next_state][next_action] * (not done)\n",
        "        q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "    else:\n",
        "        # Mise à jour Q-Learning\n",
        "        best_next_action = np.argmax(q_table[next_state])\n",
        "        target = reward + gamma * q_table[next_state][best_next_action] * (not done)\n",
        "        q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "\n",
        "    visits[state][action] += 1\n",
        "\n",
        "def train(num_episodes=800000, method=\"q_learning\"):\n",
        "    \"\"\"\n",
        "    Entraîne l'agent avec un Focus Mode après 500K épisodes.\n",
        "    \"\"\"\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        action = choose_action(state, episode)\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            next_action = choose_action(next_state, episode) if method == \"sarsa\" else None\n",
        "            update_q_table(state, action, reward, next_state, next_action, done=done, episode=episode, method=method)\n",
        "            state, action = next_state, next_action if method == \"sarsa\" else np.argmax(q_table[next_state])\n",
        "\n",
        "        # Suivi des performances tous les 50 000 épisodes\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate = evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(num_episodes=50000, verbose=True):\n",
        "    \"\"\"\n",
        "    Évalue l'agent après l'entraînement.\n",
        "    \"\"\"\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "# Entraînement avec la méthode Q-learning\n",
        "train(num_episodes=800000, method=\"q_learning\")\n",
        "evaluate()\n",
        "\n",
        "# Entraînement avec la méthode SARSA\n",
        "train(num_episodes=800000, method=\"sarsa\")\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9A5iJX7Es7E",
        "outputId": "c7c2d6e7-13ea-426c-bf09-3d73f9d88a3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/800000, Win Rate: 39.90%\n",
            "Episode 50000/800000, Win Rate: 41.64%\n",
            "Episode 100000/800000, Win Rate: 40.36%\n",
            "Episode 150000/800000, Win Rate: 43.74%\n",
            "Episode 200000/800000, Win Rate: 43.80%\n",
            "Episode 250000/800000, Win Rate: 42.22%\n",
            "Episode 300000/800000, Win Rate: 42.86%\n",
            "Episode 350000/800000, Win Rate: 42.80%\n",
            "Episode 400000/800000, Win Rate: 42.76%\n",
            "Episode 450000/800000, Win Rate: 43.38%\n",
            "Episode 500000/800000, Win Rate: 43.48%\n",
            "Episode 550000/800000, Win Rate: 42.92%\n",
            "Episode 600000/800000, Win Rate: 42.72%\n",
            "Episode 650000/800000, Win Rate: 43.18%\n",
            "Episode 700000/800000, Win Rate: 42.60%\n",
            "Episode 750000/800000, Win Rate: 41.76%\n",
            "Performance sur 50000 épisodes: 42.94% de victoires\n",
            "Episode 0/800000, Win Rate: 42.22%\n",
            "Episode 50000/800000, Win Rate: 42.20%\n",
            "Episode 100000/800000, Win Rate: 41.58%\n",
            "Episode 150000/800000, Win Rate: 41.94%\n",
            "Episode 200000/800000, Win Rate: 42.50%\n",
            "Episode 250000/800000, Win Rate: 42.28%\n",
            "Episode 300000/800000, Win Rate: 43.12%\n",
            "Episode 350000/800000, Win Rate: 43.10%\n",
            "Episode 400000/800000, Win Rate: 42.42%\n",
            "Episode 450000/800000, Win Rate: 43.14%\n",
            "Episode 500000/800000, Win Rate: 44.98%\n",
            "Episode 550000/800000, Win Rate: 42.40%\n",
            "Episode 600000/800000, Win Rate: 42.78%\n",
            "Episode 650000/800000, Win Rate: 42.82%\n",
            "Episode 700000/800000, Win Rate: 42.64%\n",
            "Episode 750000/800000, Win Rate: 41.56%\n",
            "Performance sur 50000 épisodes: 42.87% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42868"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v3\n",
        "#Dernière version Q-Learning :\n",
        "    #- Gamma plus agressif pour des décisions plus directes\n",
        "    #- Pondération des victoires/défaites à 3\n",
        "    #- Réduction forte de epsilon après 600K épisodes\n",
        "\n",
        "# Paramètres de l'agent\n",
        "alpha_start = 0.1\n",
        "gamma = 0.94  # Encourage des décisions plus directes\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.00002\n",
        "method = \"q_learning\"\n",
        "\n",
        "# Création de l'environnement\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "epsilon = epsilon_start\n",
        "\n",
        "def choose_action(state, episode):\n",
        "#Politique epsilon-greedy avec stabilisation après 600K épisodes.\n",
        "\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, next_action=None, done=False, episode=1):\n",
        "    #Mise à jour de la Q-table avec un ajustement des récompenses et epsilon.\n",
        "\n",
        "    # Warm Start conservé\n",
        "    if state not in q_table:\n",
        "        q_table[state] = np.array([0.5, 0.7])\n",
        "\n",
        "    # Alpha diminue plus lentement\n",
        "    alpha = max(0.02, alpha_start / (1 + 0.000005 * episode))\n",
        "\n",
        "    # Réduction forte de epsilon après 600K épisodes\n",
        "    global epsilon\n",
        "    if episode > 600000:\n",
        "        epsilon = max(epsilon_min, 0.01)\n",
        "\n",
        "    # Augmenter l’impact des victoires et défaites\n",
        "    if reward > 0:\n",
        "        reward = 3  # Triple la valeur des victoires\n",
        "    elif reward < 0:\n",
        "        reward = -3  # Pénalise plus les défaites\n",
        "\n",
        "    # Mise à jour Q-Learning\n",
        "    best_next_action = np.argmax(q_table[next_state])\n",
        "    target = reward + gamma * q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "\n",
        "    visits[state][action] += 1\n",
        "\n",
        "def train(num_episodes=900000):\n",
        "    #Entraîne l'agent avec des ajustements agressifs après 600K épisodes.\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = choose_action(state, episode)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done=done, episode=episode)\n",
        "            state = next_state\n",
        "\n",
        "        # Suivi des performances tous les 50 000 épisodes\n",
        "        if episode % 50000 == 0:\n",
        "            win_rate = evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}\")\n",
        "\n",
        "def evaluate(num_episodes=50000, verbose=True):\n",
        "    #Évalue l'agent après l'entraînement.\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "# Entraînement final avec toutes les améliorations\n",
        "train(num_episodes=900000)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oTygOD7GtBl",
        "outputId": "e23ba1b3-e443-45bc-9470-48ad75611b2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/900000, Win Rate: 38.42%\n",
            "Episode 50000/900000, Win Rate: 43.10%\n",
            "Episode 100000/900000, Win Rate: 44.22%\n",
            "Episode 150000/900000, Win Rate: 43.66%\n",
            "Episode 200000/900000, Win Rate: 43.10%\n",
            "Episode 250000/900000, Win Rate: 42.68%\n",
            "Episode 300000/900000, Win Rate: 42.64%\n",
            "Episode 350000/900000, Win Rate: 44.00%\n",
            "Episode 400000/900000, Win Rate: 43.36%\n",
            "Episode 450000/900000, Win Rate: 42.80%\n",
            "Episode 500000/900000, Win Rate: 42.78%\n",
            "Episode 550000/900000, Win Rate: 43.62%\n",
            "Episode 600000/900000, Win Rate: 43.50%\n",
            "Episode 650000/900000, Win Rate: 42.34%\n",
            "Episode 700000/900000, Win Rate: 41.14%\n",
            "Episode 750000/900000, Win Rate: 42.68%\n",
            "Episode 800000/900000, Win Rate: 42.36%\n",
            "Episode 850000/900000, Win Rate: 42.34%\n",
            "Performance sur 50000 épisodes: 43.11% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43106"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}