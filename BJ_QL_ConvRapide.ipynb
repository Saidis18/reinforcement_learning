{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PeNJNtC7IaTS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v1\n",
        "#Agent Q-Learning optimisé pour une convergence rapide.\n",
        "    #- Décroissance exponentielle de epsilon\n",
        "    #- Décroissance logarithmique de alpha\n",
        "#Paramètres de l'agent\n",
        "alpha_start = 0.1\n",
        "gamma = 0.99  # Facteur de réduction (discount)\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.0001\n",
        "method = \"q_learning\"\n",
        "\n",
        "# Création de l'environnement\n",
        "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
        "\n",
        "q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "visits = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "epsilon = epsilon_start\n",
        "\n",
        "def choose_action(state):\n",
        "    #Politique epsilon-greedy avec décroissance rapide de epsilon.\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()  #Exploration\n",
        "    return np.argmax(q_table[state])  #Exploitation\n",
        "\n",
        "def update_q_table(state, action, reward, next_state, next_action=None, done=False, episode=1):\n",
        "    #Mise à jour de la Q-table avec une décroissance rapide de epsilon et logarithmique de alpha.\n",
        "\n",
        "    # Learning rate logarithmique\n",
        "    alpha = max(0.02, alpha_start / (1 + np.log(1 + episode)))\n",
        "\n",
        "    # Exploration décroissante exponentiellement\n",
        "    global epsilon\n",
        "    epsilon = max(epsilon_min, epsilon_start * np.exp(-epsilon_decay * episode))\n",
        "\n",
        "    # Mise à jour Q-Learning\n",
        "    best_next_action = np.argmax(q_table[next_state])\n",
        "    target = reward + gamma * q_table[next_state][best_next_action] * (not done)\n",
        "    q_table[state][action] += alpha * (target - q_table[state][action])\n",
        "\n",
        "    # Comptabilisation des visites\n",
        "    visits[state][action] += 1\n",
        "\n",
        "def train(num_episodes=100000):\n",
        "    #Entraîne l'agent Q-Learning.\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = choose_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            update_q_table(state, action, reward, next_state, done=done, episode=episode)\n",
        "            state = next_state\n",
        "\n",
        "        # Affichage périodique des performances\n",
        "        if episode % 10000 == 0:\n",
        "            win_rate = evaluate(5000, verbose=False)\n",
        "            print(f\"Episode {episode}/{num_episodes}, Win Rate: {win_rate:.2%}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "def evaluate(num_episodes=5000, verbose=True):\n",
        "    #Évalue l'agent après l'entraînement.\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])  # Exploitation totale\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "        elif reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    win_rate = wins / num_episodes\n",
        "    if verbose:\n",
        "        print(f\"Performance sur {num_episodes} épisodes: {win_rate:.2%} de victoires\")\n",
        "    return win_rate\n",
        "\n",
        "train(num_episodes=100000)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlUXGu--IwQV",
        "outputId": "dc30b973-6531-40e5-9c20-b943f41f38e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/100000, Win Rate: 39.52%, Epsilon: 1.0000\n",
            "Episode 10000/100000, Win Rate: 43.04%, Epsilon: 0.3679\n",
            "Episode 20000/100000, Win Rate: 42.22%, Epsilon: 0.1353\n",
            "Episode 30000/100000, Win Rate: 41.72%, Epsilon: 0.0498\n",
            "Episode 40000/100000, Win Rate: 44.14%, Epsilon: 0.0183\n",
            "Episode 50000/100000, Win Rate: 43.60%, Epsilon: 0.0100\n",
            "Episode 60000/100000, Win Rate: 41.80%, Epsilon: 0.0100\n",
            "Episode 70000/100000, Win Rate: 43.14%, Epsilon: 0.0100\n",
            "Episode 80000/100000, Win Rate: 42.46%, Epsilon: 0.0100\n",
            "Episode 90000/100000, Win Rate: 42.74%, Epsilon: 0.0100\n",
            "Performance sur 5000 épisodes: 42.90% de victoires\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.429"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}